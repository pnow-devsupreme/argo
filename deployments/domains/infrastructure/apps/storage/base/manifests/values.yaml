
# Installs a debugging toolbox deployment
toolbox:
  # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
  enabled: false
  # -- Toolbox image, defaults to the image used by the Ceph cluster
  image: #quay.io/ceph/ceph:v19.2.0
  # -- Toolbox tolerations
  tolerations: []
  # -- Toolbox affinity
  affinity: {}
  # -- Toolbox container security context
  containerSecurityContext:
    runAsNonRoot: true
    runAsUser: 2016
    runAsGroup: 2016
    capabilities:
      drop: ["ALL"]
  # -- Toolbox resources
  resources:
    limits:
      memory: "1Gi"
    requests:
      cpu: "100m"
      memory: "128Mi"
# Main cluster configuration
operatorNamespace: rook-ceph
clusterName: rook-ceph
monitoring:
  # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
  # Monitoring requires Prometheus to be pre-installed
  enabled: false
  # -- Whether to disable the metrics reported by Ceph. If false, the prometheus mgr module and Ceph exporter are enabled
  metricsDisabled: false
  # -- Whether to create the Prometheus rules for Ceph alerts
  createPrometheusRules: false
  # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
  # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
  # deployed) to set rulesNamespaceOverride for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
  rulesNamespaceOverride:
  # Monitoring settings for external clusters:
  # externalMgrEndpoints: <list of endpoints>
  # externalMgrPrometheusPort: <port>
  # Scrape interval for prometheus
  # interval: 10s
  # allow adding custom labels and annotations to the prometheus rule
  prometheusRule:
    # -- Labels applied to PrometheusRule
    labels: {}
    # -- Annotations applied to PrometheusRule
    annotations: {}

# Main Ceph cluster specification
cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  # Single node configuration
  mon:
    count: 3
    allowMultiplePerNode: true
  mgr:
    count: 3
    allowMultiplePerNode: true
  dashboard:
    enabled: true
    ssl: false
    port: 7000
  network:
    provider: host
  # Storage configuration
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: "k8s-master.pnhyd.local" # Replace with your node name
        devices:
          # First disk - sdb partitions
          - name: "/dev/sdb1"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdb2"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdb3"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdb4"
            config:
              osdsPerDevice: "4"
          # Second disk - sdc partitions
          - name: "/dev/sdc1"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdc2"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdc3"
            config:
              osdsPerDevice: "4"
          - name: "/dev/sdc4"
            config:
              osdsPerDevice: "4"
  # Resource configuration
  resources:
    mgr:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    mon:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    osd:
      limits:
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
  # Placement for single node
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
# Storage Pools Configuration
cephBlockPools:
  - name: replicapool
    spec:
      failureDomain: osd # Using OSD as failure domain since all on same node
      replicated:
        size: 1 # No replication as requested
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
cephFileSystems:
  - name: ceph-fs
    spec:
      metadataPool:
        replicated:
          size: 1
      dataPools:
        - name: data0
          replicated:
            size: 1
      metadataServer:
        activeCount: 1
        activeStandby: true
    storageClass:
      enabled: true
      name: ceph-filesystem
      reclaimPolicy: Delete
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
# Ingress configuration for Ceph Dashboard
ingress:
  dashboard:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-verify: "false"
    ingressClassName: nginx
    host:
      name: ceph.proficientnowtech.com
      path: /
      pathType: Prefix
    tls:
      - hosts:
          - ceph.proficientnowtech.com
        secretName: ceph-dashboard-tls
