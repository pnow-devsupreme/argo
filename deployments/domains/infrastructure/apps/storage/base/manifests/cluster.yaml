apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  source:
    repoURL: 'https://charts.rook.io/release'
    targetRevision: v1.15.7
    chart: rook-ceph-cluster
    helm:
      values: |-
        # Installs a debugging toolbox deployment
        toolbox:
          enabled: false
          image: quay.io/ceph/ceph:v19.2.0
          containerSecurityContext:
            runAsNonRoot: true
            runAsUser: 2016
            runAsGroup: 2016
            capabilities:
              drop: ["ALL"]
          resources:
            limits:
              memory: "1Gi"
            requests:
              cpu: "100m"
              memory: "128Mi"
        # Main Ceph cluster specification
        cephClusterSpec:
          cephVersion:
            image: quay.io/ceph/ceph:v19.2.0
            allowUnsupported: false
          dataDirHostPath: /var/lib/rook
          # Multi node configuration
          mon:
            count: 1
            allowMultiplePerNode: true
          mgr:
            count: 1
            allowMultiplePerNode: true
          dashboard:
            enabled: true
            ssl: false
            port: 7000
          network:
            provider: host
          # Storage configuration
          storage:
            useAllNodes: false
            useAllDevices: false
            nodes:
              - name: "k8s-master.pnhyd.local"
                devices:
                  # First disk - sdb partitions
                  - name: "/dev/sdb1"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdb2"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdb3"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdb4"
                    config:
                      osdsPerDevice: "4"
                  # Second disk - sdc partitions
                  - name: "/dev/sdc1"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdc2"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdc3"
                    config:
                      osdsPerDevice: "4"
                  - name: "/dev/sdc4"
                    config:
                      osdsPerDevice: "4"
          # Resource configuration
          resources:
            mgr:
              limits:
                memory: "1Gi"
              requests:
                cpu: "500m"
                memory: "512Mi"
            mon:
              limits:
                memory: "1Gi"
              requests:
                cpu: "500m"
                memory: "512Mi"
            osd:
              limits:
                memory: "2Gi"
              requests:
                cpu: "500m"
                memory: "1Gi"
          # Placement for single node
          placement:
            all:
              nodeSelector:
                kubernetes.io/hostname: "k8s-master.pnhyd.local"
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: node-role.kubernetes.io/master
                  operator: Exists
                  effect: NoSchedule
        # Storage Pools Configuration
        cephBlockPools:
          - name: replicapool
            spec:
              failureDomain: osd # Using OSD as failure domain since all on same node
              replicated:
                size: 1 # No replication as requested
            storageClass:
              enabled: true
              name: ceph-block
              isDefault: true
              reclaimPolicy: Delete
              allowVolumeExpansion: true
              parameters:
                imageFormat: "2"
                imageFeatures: layering
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/fstype: ext4
        cephFileSystems:
          - name: ceph-fs
            spec:
              metadataPool:
                replicated:
                  size: 1
              dataPools:
                - name: data0
                  replicated:
                    size: 1
              metadataServer:
                activeCount: 1
                activeStandby: true
            storageClass:
              enabled: true
              name: ceph-filesystem
              reclaimPolicy: Delete
              parameters:
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                csi.storage.k8s.io/node-stage-secret-namespace: "rook-ceph"
        # Ingress configuration for Ceph Dashboard
        ingress:
          dashboard:
            enabled: true
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
              nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
              nginx.ingress.kubernetes.io/ssl-verify: "false"
            ingressClassName: nginx
            host:
              name: ceph.proficientnowtech.com
              path: /
              pathType: Prefix
            tls:
              - hosts:
                  - ceph.proficientnowtech.com
                secretName: ceph-dashboard-tls
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
